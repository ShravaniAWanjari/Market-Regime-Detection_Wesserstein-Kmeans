{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Imports\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from collections import deque\n",
        "import random\n",
        "import datetime\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "import yfinance as yf\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "zAtubpebkBZm"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load & Prepare Data\n",
        "\n",
        "# Load CSVs\n",
        "df_regime = pd.read_csv('regime.csv', parse_dates=['date'])\n",
        "df_price = pd.read_csv('price.csv', parse_dates=['date'])\n",
        "\n",
        "# Merge on date\n",
        "df = pd.merge(df_regime, df_price, on='date')\n",
        "df.sort_values('date', inplace=True)\n",
        "\n",
        "# Calculate absolute prediction error\n",
        "df['error'] = np.abs(df['actual_price'] - df['predicted_price'])\n",
        "\n",
        "# Compute 7-day rolling average error\n",
        "df['rolling_error'] = df['error'].rolling(window=7).mean()\n",
        "\n",
        "# Drop initial rows where rolling average is NaN\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "# Preview\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "VJnFyuqLkgTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2 (Alternative): Generate mock regime and price data without CSVs\n",
        "\n",
        "\n",
        "# Generate 200 days of mock data\n",
        "np.random.seed(42)\n",
        "n_days = 200\n",
        "dates = pd.date_range(end=pd.Timestamp.today(), periods=n_days)\n",
        "\n",
        "# Simulate actual prices (e.g., trending stock with noise)\n",
        "actual_price = np.cumsum(np.random.randn(n_days) * 2 + 0.5) + 100\n",
        "\n",
        "# Simulate predicted prices with some noise\n",
        "predicted_price = actual_price * (1 + np.random.normal(0, 0.02, n_days))\n",
        "\n",
        "# Simulate regime based on price movement\n",
        "price_diff = np.diff(actual_price, prepend=actual_price[0])\n",
        "regime = np.where(price_diff > 1, 1, np.where(price_diff < -1, -1, 0))\n",
        "\n",
        "# Construct mock DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'date': dates,\n",
        "    'regime': regime,\n",
        "    'actual_price': actual_price,\n",
        "    'predicted_price': predicted_price\n",
        "})\n",
        "\n",
        "# Compute error and 7-day rolling error\n",
        "df['error'] = np.abs(df['actual_price'] - df['predicted_price'])\n",
        "df['rolling_error'] = df['error'].rolling(window=7).mean()\n",
        "\n",
        "# Drop rows with NaN (from rolling average)\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "# Preview\n",
        "#df.head()\n"
      ],
      "metadata": {
        "id": "y-o00MSNkqvj"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TradingEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom Trading Environment for RL Agent.\n",
        "    Observations:\n",
        "        [regime (1/0/-1), predicted_price, 7-day average prediction error]\n",
        "    Actions:\n",
        "        0: Buy, 1: Hold, 2: Sell\n",
        "    \"\"\"\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, merged_df, lookback=7):\n",
        "        super(TradingEnv, self).__init__()\n",
        "        self.data = merged_df.copy()\n",
        "        self.lookback = lookback\n",
        "        self.data['error'] = np.abs(self.data['actual_price'] - self.data['predicted_price'])\n",
        "        self.max_error = self.data['error'].max()\n",
        "\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(3,), dtype=np.float32)\n",
        "        self.reset()\n",
        "\n",
        "    def _get_obs(self):\n",
        "        row = self.data.iloc[self.current_step]\n",
        "        regime = row['regime']\n",
        "        predicted = row['predicted_price']\n",
        "        avg_error = self.data.iloc[self.current_step - self.lookback:self.current_step]['error'].mean()\n",
        "        return np.array([regime, predicted, avg_error], dtype=np.float32)\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = self.lookback\n",
        "        return self._get_obs()\n",
        "\n",
        "    def step(self, action):\n",
        "        row = self.data.iloc[self.current_step]\n",
        "        regime = row['regime']\n",
        "\n",
        "        # Base reward matrix\n",
        "        if regime == 1:  # Bull\n",
        "            base_reward = 1.0 if action == 0 else (0.0 if action == 1 else -1.0)\n",
        "        elif regime == -1:  # Bear\n",
        "            base_reward = 1.0 if action == 2 else (0.0 if action == 1 else -1.0)\n",
        "        else:  # Neutral\n",
        "            base_reward = 1.0 if action == 1 else -0.1\n",
        "\n",
        "        # Scaled penalty from prediction error\n",
        "        obs = self._get_obs()\n",
        "        avg_error = obs[2]\n",
        "        alpha = 1.0\n",
        "        error_penalty = alpha * (avg_error / (self.max_error + 1e-6))\n",
        "\n",
        "        reward = base_reward - error_penalty\n",
        "\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= len(self.data)\n",
        "        next_state = self._get_obs() if not done else np.zeros(self.observation_space.shape)\n",
        "        return next_state, reward, done, {}\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        obs = self._get_obs()\n",
        "        print(f\"Step {self.current_step}: Regime={obs[0]}, Predicted={obs[1]:.2f}, Avg Error={obs[2]:.2f}\")\n"
      ],
      "metadata": {
        "id": "SCu_Or4Pk2sj"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size  # 3: regime, predicted_price, avg_error\n",
        "        self.action_size = action_size  # 3: Buy, Hold, Sell\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95    # discount factor\n",
        "        self.epsilon = 1.0   # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state[np.newaxis, :], verbose=0)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target += self.gamma * np.amax(self.model.predict(next_state[np.newaxis, :], verbose=0)[0])\n",
        "            target_f = self.model.predict(state[np.newaxis, :], verbose=0)\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state[np.newaxis, :], target_f, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n"
      ],
      "metadata": {
        "id": "YgrAoTx3k4UP"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = TradingEnv(df)\n",
        "agent = DQNAgent(state_size=3, action_size=3)\n",
        "episodes = 50\n",
        "batch_size = 32\n",
        "rewards_history = []\n",
        "\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if len(agent.memory) > batch_size:\n",
        "            agent.replay(batch_size)\n",
        "\n",
        "    rewards_history.append(total_reward)\n",
        "    print(f\"Episode {e+1}/{episodes} - Total Reward: {total_reward:.2f} - Epsilon: {agent.epsilon:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcKmy7B2prCH",
        "outputId": "ab21bd92-b4f4-4111-ec4e-cb45bf53b4e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/50 - Total Reward: -1.49 - Epsilon: 0.46\n",
            "Episode 2/50 - Total Reward: 49.31 - Epsilon: 0.18\n",
            "Episode 3/50 - Total Reward: 96.11 - Epsilon: 0.07\n",
            "Episode 4/50 - Total Reward: 123.51 - Epsilon: 0.03\n",
            "Episode 5/50 - Total Reward: 118.01 - Epsilon: 0.01\n",
            "Episode 6/50 - Total Reward: 132.91 - Epsilon: 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def animate_training_episode(env, agent):\n",
        "    saved_epsilon = agent.epsilon\n",
        "    agent.epsilon = 0.0  # deterministic policy\n",
        "\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    step_log = []\n",
        "\n",
        "    while not done:\n",
        "        action = agent.act(state)\n",
        "        reward = 0\n",
        "        base_reward = 0\n",
        "        regime = state[0]\n",
        "        if regime == 1: base_reward = 1.0 if action == 0 else (0.0 if action == 1 else -1.0)\n",
        "        elif regime == -1: base_reward = 1.0 if action == 2 else (0.0 if action == 1 else -1.0)\n",
        "        else: base_reward = 1.0 if action == 1 else -0.1\n",
        "        penalty = state[2] / (env.max_error + 1e-6)\n",
        "        reward = base_reward - penalty\n",
        "\n",
        "        step_log.append({\n",
        "            'step': env.current_step,\n",
        "            'predicted': state[1],\n",
        "            'action': action,\n",
        "            'penalty': penalty,\n",
        "            'reward': reward\n",
        "        })\n",
        "\n",
        "        next_state, _, done, _ = env.step(action)\n",
        "        state = next_state\n",
        "\n",
        "    agent.epsilon = saved_epsilon\n",
        "\n",
        "    # Plot Animation\n",
        "    steps = [d['step'] for d in step_log]\n",
        "    predicted = [d['predicted'] for d in step_log]\n",
        "    actions = [d['action'] for d in step_log]\n",
        "    penalties = [d['penalty'] for d in step_log]\n",
        "    rewards = [d['reward'] for d in step_log]\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
        "    colors = {0: 'green', 1: 'blue', 2: 'red'}\n",
        "\n",
        "    def init():\n",
        "        ax1.clear()\n",
        "        ax2.clear()\n",
        "        ax1.set_title(\"Predicted Price with Action Markers\")\n",
        "        ax2.set_title(\"Penalty and Reward Over Time\")\n",
        "        ax1.set_ylabel(\"Predicted Price\")\n",
        "        ax2.set_ylabel(\"Value\")\n",
        "        ax2.set_xlabel(\"Step\")\n",
        "\n",
        "    def update(i):\n",
        "        ax1.clear()\n",
        "        ax2.clear()\n",
        "        ax1.plot(steps[:i+1], predicted[:i+1], 'k--')\n",
        "        for j in range(i+1):\n",
        "            ax1.scatter(steps[j], predicted[j], color=colors[actions[j]], s=60)\n",
        "        ax2.plot(steps[:i+1], penalties[:i+1], 'r-', label='Penalty')\n",
        "        ax2.plot(steps[:i+1], rewards[:i+1], 'b-', label='Final Reward')\n",
        "        ax2.legend()\n",
        "\n",
        "    ani = animation.FuncAnimation(fig, update, frames=len(steps), init_func=init,\n",
        "                                  interval=300, repeat=False)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "animate_training_episode(env, agent)\n"
      ],
      "metadata": {
        "id": "d7nc9iI8s3bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(env.data['date'], env.data['error'].rolling(7).mean(), label='7-Day Avg Error')\n",
        "plt.title(\"Rolling 7-Day Prediction Error\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FGfGWsias5F4"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-run one deterministic episode to collect actions\n",
        "state = env.reset()\n",
        "agent.epsilon = 0.0\n",
        "done = False\n",
        "decision_points = []\n",
        "\n",
        "while not done:\n",
        "    action = agent.act(state)\n",
        "    decision_points.append((env.data.iloc[env.current_step]['date'],\n",
        "                            env.data.iloc[env.current_step]['actual_price'],\n",
        "                            action))\n",
        "    next_state, _, done, _ = env.step(action)\n",
        "    state = next_state\n",
        "\n",
        "# Plotting decisions\n",
        "dates, prices, actions = zip(*decision_points)\n",
        "colors = ['green' if a == 0 else 'blue' if a == 1 else 'red' for a in actions]\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(env.data['date'], env.data['actual_price'], label='Actual Price', color='black')\n",
        "plt.scatter(dates, prices, c=colors, label='Actions', s=50, alpha=0.7)\n",
        "plt.title(\"Buy (Green), Hold (Blue), Sell (Red) Decisions\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IXV96GvMwlw3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}